{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "Copyright (c) 2017 Andrew Glassner\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "</small>\n",
    "\n",
    "\n",
    "\n",
    "# Deep Learning From Basics to Practice\n",
    "## by Andrew Glassner, https://dlbasics.com, http://glassner.com\n",
    "------\n",
    "## Chapter 21: Convolutional Neural Networks (CNNs)\n",
    "### Notebook 4: Filter activations in VGG16\n",
    "\n",
    "This notebook is provided as a “behind-the-scenes” look at code used to make some of the figures in this chapter. It is still in the hacked-together form used to develop the figures, and is only lightly commented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This code is adapted from https://github.com/fchollet/deep-learning-with-python-notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.applications import VGG16\n",
    "from keras import backend as K_backend\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Just in case the Keras defaults aren't as we expect\n",
    "K_backend.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a File_Helper for saving and loading files.\n",
    "\n",
    "save_files = True\n",
    "\n",
    "import os, sys, inspect\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "sys.path.insert(0, os.path.dirname(current_dir)) # path to parent dir\n",
    "from DLBasics_Utilities import File_Helper\n",
    "file_helper = File_Helper(save_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_VGG16():\n",
    "    model = VGG16(weights='imagenet', include_top=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_image_for_display(image):\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    image -= image.mean()\n",
    "    image /= (image.std() + 1e-5)\n",
    "    image *= 0.1\n",
    "    image += 0.5\n",
    "\n",
    "    # convert to RGB array\n",
    "    image *= 255\n",
    "    image = np.clip(image, 0, 255).astype('uint8')\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where the Keras magic happens.\n",
    "# From https://github.com/fchollet/deep-learning-with-python-notebooks\n",
    "def get_filter_image(model, layer_name, filter_index, size=150, num_steps=40):\n",
    "    # Build a loss function that maximizes the activation\n",
    "    # of the nth filter of the layer considered. We find the\n",
    "    # average value of all the activations coming out of the filter.\n",
    "    layer_output = model.get_layer(layer_name).output\n",
    "    loss = K_backend.mean(layer_output[:, :, :, filter_index])\n",
    "\n",
    "    # Compute the gradient of the input picture wrt this loss\n",
    "    grads = K_backend.gradients(loss, model.input)[0]\n",
    "\n",
    "    # Normalization trick: we normalize the gradient\n",
    "    grads /= (K_backend.sqrt(K_backend.mean(K_backend.square(grads))) + 1e-5)\n",
    "\n",
    "    # This function returns the loss and grads given the input picture\n",
    "    iterate = K_backend.function([model.input], [loss, grads])\n",
    "    \n",
    "    # We start from a gray image with some noise\n",
    "    input_img_data = np.random.random((1, size, size, 3)) * 20 + 128.\n",
    "\n",
    "    # Run gradient ascent \n",
    "    step_size = 1.\n",
    "    for i in range(num_steps):\n",
    "        loss_value, grads_value = iterate([input_img_data])\n",
    "        input_img_data += grads_value * step_size\n",
    "        \n",
    "    img = input_img_data[0]\n",
    "    return prepare_image_for_display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_one_filter(model, layer_name, filter_number, size=150, filename=None):\n",
    "    filter_image = get_filter_image(model, layer_name, filter_number, size)\n",
    "    plt.imshow(filter_image)\n",
    "    plt.xticks([],[])\n",
    "    plt.yticks([],[])\n",
    "    plt.title(layer_name)\n",
    "    file_helper.save_figure(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a bunch of filters from the given layer\n",
    "def show_filter_grid(model, layer_name, filters_list, num_rows, num_cols, filename=None):\n",
    "    img_size = 64\n",
    "    gap_size = 3\n",
    "    \n",
    "    pixels_wide = (num_cols * img_size) + ((num_cols-1) * gap_size)\n",
    "    pixels_high = (num_rows * img_size) + ((num_rows-1) * gap_size)\n",
    "\n",
    "    grid = np.zeros((pixels_high, pixels_wide, 3))\n",
    "    \n",
    "    for img_num, filter_number in enumerate(filters_list):\n",
    "        filter_image = get_filter_image(model, layer_name, filter_number, size=img_size)\n",
    "        y = img_num // num_cols\n",
    "        x = img_num - (y*num_cols)\n",
    "        h_start = x * (img_size + gap_size)\n",
    "        v_start = y * (img_size + gap_size)\n",
    "        h_end = h_start + img_size\n",
    "        v_end = v_start + img_size\n",
    "        #print(\"img_num=\",img_num,\" filter_number=\",filter_number,\" y=\",y,\" x=\",x)\n",
    "        grid[v_start:v_end, h_start:h_end, :] = filter_image\n",
    "    \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(grid)\n",
    "    plt.title('Filters from VGG16 layer '+layer_name, fontsize=28, y=1.01)\n",
    "    plt.xticks([],[])\n",
    "    plt.yticks([],[])\n",
    "    file_helper.save_figure(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# almost identical to show_filter_grid, but shows only selected layer/filter pairs\n",
    "def show_selections_grid(model, layer_filter_list, num_rows, num_cols, filename=None):\n",
    "    img_size = 150\n",
    "    gap_size = 3\n",
    "    \n",
    "    pixels_wide = (num_cols * img_size) + ((num_cols-1) * gap_size)\n",
    "    pixels_high = (num_rows * img_size) + ((num_rows-1) * gap_size)\n",
    "\n",
    "    grid = np.zeros((pixels_high, pixels_wide, 3))\n",
    "    \n",
    "    for img_num, filter_pair in enumerate(layer_filter_list):\n",
    "        layer_name = filter_pair[0]\n",
    "        filter_number = filter_pair[1]\n",
    "        filter_image = get_filter_image(model, layer_name, filter_number, size=img_size)\n",
    "        y = img_num // num_cols\n",
    "        x = img_num - (y*num_cols)\n",
    "        h_start = x * (img_size + gap_size)\n",
    "        v_start = y * (img_size + gap_size)\n",
    "        h_end = h_start + img_size\n",
    "        v_end = v_start + img_size\n",
    "        #print(\"img_num=\",img_num,\" filter_number=\",filter_number,\" y=\",y,\" x=\",x)\n",
    "        grid[v_start:v_end, h_start:h_end, :] = filter_image\n",
    "    \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(grid)\n",
    "    plt.title('Selected filters from VGG16', fontsize=28, y=1.01)\n",
    "    plt.xticks([],[])\n",
    "    plt.yticks([],[])\n",
    "    file_helper.save_figure(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_VGG16()\n",
    "# Let's see what the layers are named, so we can refer to them\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_one_filter(model, 'block1_conv1', 0, size=150, filename='block1_conv1-filter-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Since grids bigger than 8 by 8 get too small for detail, we \n",
    "# only show the first 64 filters from each layer.\n",
    "#\n",
    "# Structure of the lists:\n",
    "# block number, conv number, #filters to show, num_rows, num_cols\n",
    "\n",
    "filter_sets = [\n",
    "    [1, 1,  64,  8,  8 ],\n",
    "    [1, 2,  64,  8,  8 ],\n",
    "    [2, 1,  64,  8,  8 ],\n",
    "    [2, 2,  64,  8,  8 ],\n",
    "    [3, 1,  64,  8,  8 ],\n",
    "    [3, 2,  64,  8,  8 ],\n",
    "    [3, 3,  64,  8,  8 ],\n",
    "    [4, 1,  64,  8,  8 ],\n",
    "    [4, 2,  64,  8,  8 ],\n",
    "    [4, 3,  64,  8,  8 ],\n",
    "    [5, 1,  64,  8,  8 ],\n",
    "    [5, 2,  64,  8,  8 ],\n",
    "    [5, 3,  64,  8,  8 ],\n",
    "]\n",
    "\n",
    "for set in filter_sets:\n",
    "    block_num, conv_num, num_filters, num_rows, num_cols = set\n",
    "    layer_name = 'block'+str(block_num)+'_conv'+str(conv_num)\n",
    "    file_name = 'block'+str(block_num)+'-filter'+str(conv_num)+'-size-'+str(num_filters)\n",
    "    #print(\"layer_name=\",layer_name,\" file_name=\",file_name)\n",
    "    show_filter_grid(model, layer_name, range(num_filters), num_rows, num_cols, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_filter_list_1 = [\n",
    "    ['block1_conv1', 21],\n",
    "    ['block1_conv2', 12],\n",
    "    ['block2_conv2', 29],\n",
    "    ['block3_conv1', 17],\n",
    "    ['block3_conv1', 28],\n",
    "    ['block3_conv1', 59],\n",
    "    ['block3_conv2', 5],\n",
    "    ['block3_conv2', 8],\n",
    "    ['block3_conv3', 20]\n",
    "]\n",
    "\n",
    "show_selections_grid(model, layer_filter_list_1, 3, 3, filename='VGG16-selections-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_filter_list_2 = [\n",
    "    ['block3_conv3', 21],\n",
    "    ['block4_conv1', 0],\n",
    "    ['block4_conv1', 17],\n",
    "    ['block4_conv1', 53],\n",
    "    ['block4_conv1', 47],\n",
    "    ['block4_conv2', 27],\n",
    "    ['block5_conv1', 48],\n",
    "    ['block5_conv2', 25],\n",
    "    ['block5_conv2', 57]\n",
    "]\n",
    "\n",
    "show_selections_grid(model, layer_filter_list_2, 3, 3, filename='VGG16-selections-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
